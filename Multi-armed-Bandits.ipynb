{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4496662c",
   "metadata": {},
   "source": [
    "# Multi-armed Bandits Algorithms\n",
    "\n",
    "This notebook explains and reproduces the most popular multi-armed bandits (MAB) algorithms:\n",
    "- epsilon-greedy\n",
    "- UCB\n",
    "- Thompson Sampling\n",
    "\n",
    "### What?\n",
    "MAB is a classic reinforcement learning problem for balancing the trade-off between exploration vs exploitation.\n",
    "\n",
    "Essentially, MAB formulates various kinds of sequential decision-making problems; thus, it is applicable to a wide range of scenarios.\n",
    "\n",
    "### Example\n",
    "In a casino, a gambler plays a slot machine that has `K` pulling arms (the story if there are `K` slot machines with one arm). \n",
    "\n",
    "How to leave the casino without being bankrupt? Or even with some earnings?\n",
    "\n",
    "##### Rules\n",
    "1. Each arm has a reward distribution that is initially unknown to the gambler.\n",
    "2. Each round, the gambler can only play one arm and only observe the reward of that very arm.\n",
    "\n",
    "##### Goal\n",
    "Maximize the cumulative reward before he bankrupts or the casino closes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b9d810",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "04befc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bb0f59",
   "metadata": {},
   "source": [
    "# Problem (Bandit Environment)\n",
    "\n",
    "Don't get confused.\n",
    "\n",
    "MAB algorithms solves MAB problems.\n",
    "\n",
    "We need to define a problem first before investigating how each algorithms perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9fc9b5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractMAPProblem(object):\n",
    "    \"\"\"\n",
    "    The basics of a generic MAB problem\n",
    "    \"\"\"\n",
    "    def __init__(self, name: str, num_arms: int):\n",
    "        self.bandit_name = name\n",
    "        self.num_arms = num_arms\n",
    "\n",
    "    def get_reward(self, arm_choice: int):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b304aa",
   "metadata": {},
   "source": [
    "### The gambler, Jack\n",
    "In front of Jack, the gambler, is a fancy slot machine, called the Fruit Machine.\n",
    "```\n",
    "  ______          _ _     __  __            _     _            \n",
    " |  ____|        (_) |   |  \\/  |          | |   (_)           \n",
    " | |__ _ __ _   _ _| |_  | \\  / | __ _  ___| |__  _ _ __   ___ \n",
    " |  __| '__| | | | | __| | |\\/| |/ _` |/ __| '_ \\| | '_ \\ / _ \\\n",
    " | |  | |  | |_| | | |_  | |  | | (_| | (__| | | | | | | |  __/\n",
    " |_|  |_|   \\__,_|_|\\__| |_|  |_|\\__,_|\\___|_| |_|_|_| |_|\\___|\n",
    "```\n",
    "\n",
    "##### Fruit Machine\n",
    "There are 3 arms/buttons that correspond to 3 different fruits (cherry, orange, and manage).\n",
    "\n",
    "Each time Jack pays $1 to play.\n",
    "\n",
    "This machine only requires one single brain cell to play:\n",
    "1. choose a fruit & pull its arm -> the wheel starts to spin -> the wheel stops\n",
    "2. win the prize (get back $2) or lose ($1 is gone)\n",
    "3. play again (going home is never an option for Jack :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "619a9f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FruitSlotMachine(AbstractMAPProblem):\n",
    "    \"\"\"\n",
    "    An Bernoulli Bandit. Reward obeys a Bernoulli distribution\n",
    "    This machine has 3 arms:\n",
    "    0 -- cherry\n",
    "    1 -- orange\n",
    "    2 -- mango\n",
    "    \"\"\"\n",
    "    def __init__(self, distribution: list = None):\n",
    "        \"\"\"\n",
    "        this machine supports two factory modes:\n",
    "        1. set a fixed win rate for each arm\n",
    "        2. randomize the win rate for each arm\n",
    "        \"\"\"\n",
    "        super().__init__(\"Fruit Machine\", 3)\n",
    "        if distribution is not None: # mode 1\n",
    "            assert len(distribution) == self.num_arms\n",
    "            self.distribution = distribution\n",
    "        else: # mode 2\n",
    "            self.distribution = [np.random.random() for _ in range(self.num_arms)]\n",
    "        \n",
    "        self.best_cumulative_reward = max(self.distribution)\n",
    "\n",
    "    def get_reward(self, arm_choice: int):\n",
    "        \"\"\"\n",
    "        return the sampled reward for the given arm choice\n",
    "        \n",
    "        Output:\n",
    "        reward -- 0 or 1\n",
    "        \"\"\"\n",
    "        if np.random.random() < self.distribution[arm_choice]:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92ed314",
   "metadata": {},
   "source": [
    "As we are the owner of the Fruit Machine, we can adjust the win rate:\n",
    "- cherry -> 30% win, 70% lose\n",
    "- orange -> 50% win, 50% lose\n",
    "- manago -> 70% win, 30% lose\n",
    "\n",
    "Obviously, we know betting on manago is always the best.\n",
    "Gamblers need to play tons of times before they eventually realize this secret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2b0340f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Fruit Machine with a fixed win rate / distribution\n",
    "FruitMachine = FruitSlotMachine([0.3, 0.5, 0.7])\n",
    "Fruits = ['cherry', 'orange', 'mango']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9c2f94",
   "metadata": {},
   "source": [
    "# Algorithms\n",
    "An MAB algorithm solves MAB problems.\n",
    "### What does it do?\n",
    "Essentially, an MAB algorithm is a strategy (aka, policy) on how to perform the next move, such that we gain better understanding of the problem and the hidden distributions.\n",
    "The algorithm will:\n",
    "1. `choose_arm`: make a decision on actions (which fruit to bet on?)\n",
    "2. observe the result and `update_belief` based on the arm choice and result/reward.\n",
    "3. repeat for some iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1f7cbcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractMABAlgorithm:\n",
    "    def __init__(self, MABproblem: FruitSlotMachine) -> None:\n",
    "        self.MABproblem = MABproblem\n",
    "        self.N_a = np.array([0] * self.MABproblem.num_arms) # the number of choice on arm a\n",
    "        self.regret = 0 # cumulative regret\n",
    "        self.regret_history = [0] # regret history at each step\n",
    "    \n",
    "    def choose_arm(self) -> int:\n",
    "        \"\"\"choose an arm/action\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def update_belief(self, arm_choice, reward) -> None:\n",
    "        \"\"\"update the model\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def update_regret(self, arm_choice: int):\n",
    "        self.regret += self.MABproblem.best_cumulative_reward - self.MABproblem.distribution[arm_choice]\n",
    "        self.regret_history.append(self.regret)\n",
    "\n",
    "    def run(self, num_iterations: int):\n",
    "        for self.iteration in range(num_iterations):\n",
    "            # choose an arm & observe reward\n",
    "            arm_choice = self.choose_arm()\n",
    "            reward = self.MABproblem.get_reward(arm_choice)\n",
    "            # update model & regret\n",
    "            self.N_a[arm_choice] += 1\n",
    "            self.update_belief(arm_choice, reward)\n",
    "            self.update_regret(arm_choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f2ad4d",
   "metadata": {},
   "source": [
    "## Stocastic Bandits\n",
    "In the simplest form, stochastic bandits deal with IID reward model. The independent and identical distributed reward D (with an expected value μ and a variance σ) is initially hidden to the agent. At each time step t ∈ T, an agent selects one of the K arms and only observe the reward from the selected arm a, known as bandit feedback. Reward value is simplified to be 0 or 1 (or within [0, 1]). It is the goal of an agent to realize the reward distributions and maximizes the cumulative reward in T iterations.\n",
    "To access a bandit algorithm, we need to define Regret. Given IID rewards, the highest cumulative reward can be obtained by repeatedly choosing the optimal arm for T iterations.\n",
    "\n",
    "### Uniform Exploration\n",
    "The simplest strategies.\n",
    "\n",
    "Uniform Exploration ignores any observed rewards and explores each arm uniformly.\n",
    "For example, Jacky with 1000$ plays each fruit for 333 times, no matter of winning or losing.\n",
    "\n",
    "Notice Uniform Exploration is known as Explore-first algorithm. After exploration phase, simply exploit the best arm thereafter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "875b28ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniformExploration(AbstractMABAlgorithm):\n",
    "    def __init__(self, MABproblem: FruitSlotMachine, initial_probability: float=1.0) -> None:\n",
    "        super().__init__(MABproblem)\n",
    "        self.probabilities = np.array([initial_probability] * self.MABproblem.num_arms)\n",
    "\n",
    "    def choose_arm(self) -> int:\n",
    "        arm = np.argmin(self.N_a)\n",
    "        return arm\n",
    "\n",
    "    def update_belief(self, arm_choice, reward) -> None:\n",
    "        self.probabilities[arm_choice] += (reward - self.probabilities[arm_choice]) / (self.N_a[arm_choice]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fe6310",
   "metadata": {},
   "source": [
    "##### Performance\n",
    "Jack has to make a decision on how to divide the $1000 chips.\n",
    "Here, Jack decides to use $999 to explore and $1 to exploit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e7263874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.33933934 0.51351351 0.69069069]\n",
      "Approximate winrate for cherry: 0.339\n",
      "\t#times played = 333\n",
      "Approximate winrate for orange: 0.514\n",
      "\t#times played = 333\n",
      "Approximate winrate for mango: 0.691\n",
      "\t#times played = 333\n"
     ]
    }
   ],
   "source": [
    "uniform_explore = UniformExploration(FruitMachine)\n",
    "uniform_explore.run(num_iterations=999)\n",
    "print(uniform_explore.probabilities)\n",
    "for i, winrate in enumerate(uniform_explore.probabilities):\n",
    "    print(f\"Approximate winrate for {Fruits[i]}:\", round(winrate, 3))\n",
    "    print(f\"\\t#times played = {uniform_explore.N_a[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7c362c",
   "metadata": {},
   "source": [
    "##### Questions\n",
    "1. What if Jack use $500 to explore and $500 to exploit?\n",
    "2. What if too less exploration?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ca9846",
   "metadata": {},
   "source": [
    "### Epsilon-greedy\n",
    "Explores a random arm at a probability of ε, and exploit the current best arm otherwise, i.e. greedy choice.\n",
    "\n",
    "##### Epsilon value\n",
    "Cesa-Bianchi and Fischer (1998) proved that the regret has logarithmic bound if ε is proportionally to the reciprocal of iterations (i.e. ε ∝ 1t ), though a widely-cited empirical evaluation was not able to prove the practicality of this theoretically sound statement (Vermorel and Mohri, 2005)\n",
    "\n",
    "We use fixed a epsilon value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c3190a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedy(AbstractMABAlgorithm):\n",
    "    def __init__(self, MABproblem: FruitSlotMachine, epsilon: float, initial_probability: float=1.0) -> None:\n",
    "        super().__init__(MABproblem)\n",
    "        assert epsilon is None or 0 <= epsilon <= 1.0\n",
    "        self.epsilon = epsilon\n",
    "        self.probabilities = np.array([initial_probability] * self.MABproblem.num_arms)\n",
    "\n",
    "    def choose_arm(self) -> int:\n",
    "        if np.random.random() < self.epsilon:\n",
    "            arm = np.random.randint(0, self.MABproblem.num_arms)\n",
    "        else:\n",
    "            arm = self.probabilities.argmax()\n",
    "        return arm\n",
    "\n",
    "    def update_belief(self, arm_choice, reward) -> None:\n",
    "        self.probabilities[arm_choice] += (reward - self.probabilities[arm_choice]) / (self.N_a[arm_choice]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a474172c",
   "metadata": {},
   "source": [
    "##### Performance\n",
    "Jack, the gambler, plays for a 1000 times following the Epsilon Greedy algorithm and he keeps track of the winrate of each fruit/arm/action.\n",
    "\n",
    "Jack chooses to evenly splite his exploration and exploitation (50% - 50%. Approaximately 500 times for choosing randomly)\n",
    "\n",
    "Thus, `epsilon = 0.5`\n",
    "\n",
    "Jack uses epsilon greedy to successfully approximates the winrate for each fruit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f892e030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.30718954 0.47701149 0.7013373 ]\n",
      "Approximate winrate for cherry: 0.307\n",
      "\t#times played = 153\n",
      "Approximate winrate for orange: 0.477\n",
      "\t#times played = 174\n",
      "Approximate winrate for mango: 0.701\n",
      "\t#times played = 673\n"
     ]
    }
   ],
   "source": [
    "epsilon_greedy = EpsilonGreedy(FruitMachine, epsilon=0.5)\n",
    "epsilon_greedy.run(num_iterations=1000)\n",
    "print(epsilon_greedy.probabilities)\n",
    "for i, winrate in enumerate(epsilon_greedy.probabilities):\n",
    "    print(f\"Approximate winrate for {Fruits[i]}:\", round(winrate, 3))\n",
    "    print(f\"\\t#times played = {epsilon_greedy.N_a[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac6afe0",
   "metadata": {},
   "source": [
    "##### Questions\n",
    "1. How does `epsilon` affect the result? Is larger or smaller `epsilon` works better?\n",
    "2. How does `# iterations` affect the result? In other word, how does the cumulative regret change over time?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615f4865",
   "metadata": {},
   "source": [
    "### Upper Confidence Bounds (UCB)\n",
    "The basic algorithm, UCB1, tracks the number of trails n on each arm besides the expected value μ. The agent selects an arm a based on the optimistic estimates (i.e. maximum of UCB):\n",
    "\n",
    "Repetitively selecting the arm that maximizes UCB will eventually converge to the best arm a∗, which is easily proved by contradiction of UCBt(at) ≥ UCBt(a∗)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cd47d60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCB(AbstractMABAlgorithm):\n",
    "    def __init__(self, MABproblem: FruitSlotMachine, initial_probability: float=1.0) -> None:\n",
    "        super().__init__(MABproblem)\n",
    "        self.probabilities = np.array([initial_probability] * self.MABproblem.num_arms)\n",
    "\n",
    "    def choose_arm(self) -> int:\n",
    "        t = self.iteration+1\n",
    "        UCB = self.probabilities + np.sqrt(2 * np.log(t) / (self.N_a + 1))\n",
    "        arm = np.argmax(UCB)\n",
    "        return arm\n",
    "\n",
    "    def update_belief(self, arm_choice, reward) -> None:\n",
    "        self.probabilities[arm_choice] += (reward - self.probabilities[arm_choice]) / (self.N_a[arm_choice]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af694e14",
   "metadata": {},
   "source": [
    "##### Performance\n",
    "UCB can also help Jack successfully approximates the winrate for each fruit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "84819a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.26530612 0.4822695  0.6654321 ]\n",
      "Approximate winrate for cherry: 0.265\n",
      "\t#times played = 49\n",
      "Approximate winrate for orange: 0.482\n",
      "\t#times played = 141\n",
      "Approximate winrate for mango: 0.665\n",
      "\t#times played = 810\n"
     ]
    }
   ],
   "source": [
    "ucb = UCB(FruitMachine)\n",
    "ucb.run(num_iterations=1000)\n",
    "print(ucb.probabilities)\n",
    "for i, winrate in enumerate(ucb.probabilities):\n",
    "    print(f\"Approximate winrate for {Fruits[i]}:\", round(winrate, 3))\n",
    "    print(f\"\\t#times played = {ucb.N_a[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca91734",
   "metadata": {},
   "source": [
    "## Bayesian Bandits\n",
    "Previous algorithms assume the agent has no knowledge of the reward distribution; therefore, we can only make general but conservative estimations of the bound by Hoeffding’s Inequality. How would an agent improve if the prior distribution is provided?\n",
    "\n",
    "The Bayesian bandits leverage main concepts of Bayesian statistics to solve stochastic bandit problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb365bc9",
   "metadata": {},
   "source": [
    "### Bayesian UCB\n",
    "Jack studies some distribution model and expects the mean reward of each fruit follows Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dc38a4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianUCB(AbstractMABAlgorithm):\n",
    "    def __init__(self, MABproblem: FruitSlotMachine, sigma=3, initial_a=1., initial_b=1.) -> None:\n",
    "        super().__init__(MABproblem)\n",
    "        self.sigma = sigma\n",
    "        self.a = np.array([initial_a] * MABproblem.num_arms)\n",
    "        self.b = np.array([initial_b] * MABproblem.num_arms)\n",
    "\n",
    "    def getBetaStdDev(self) -> float:\n",
    "        variance = np.multiply(self.a, self.b) / np.multiply(np.square(self.a + self.b), (self.a + self.b + 1))\n",
    "        return np.sqrt(variance)\n",
    "\n",
    "    def choose_arm(self) -> int:\n",
    "        UCB = self.a / (self.a + self.b) + self.getBetaStdDev() * self.sigma\n",
    "        arm = np.argmax(UCB)\n",
    "        return arm\n",
    "\n",
    "    def update_belief(self, arm_choice, reward) -> None:\n",
    "        self.a[arm_choice] += reward\n",
    "        self.b[arm_choice] += 1 - reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcf5e66",
   "metadata": {},
   "source": [
    "##### Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dc54317d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4.   7. 678.] [  8.  11. 298.]\n"
     ]
    }
   ],
   "source": [
    "bayes_ucb = BayesianUCB(FruitMachine)\n",
    "bayes_ucb.run(num_iterations=1000)\n",
    "print(bayes_ucb.a, bayes_ucb.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c1ee76",
   "metadata": {},
   "source": [
    "### Thompson Sampling\n",
    "\n",
    "Formally, each arm a has a Bernoulli distribution (i.e. reward received is either a success or a failure), with the mean denoted by μ; it is common to model the mean μ of each arm using Beta distribution. Each arm corresponds to a random variable θ ∈ [0, 1] that represents the probability of receiving a success. The agent starts off with a given belief of the distribution (e.g. αk = 1, βk = 1 indicating the initial success rate of arm a is uniform); at each iteration t, the agent chooses an optimal arm according to the current prior probability, and the belief is updated by posterior distribution upon receiving the reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ac5e64d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThompsonSamping(AbstractMABAlgorithm):\n",
    "    def __init__(self, MABproblem: FruitSlotMachine, initial_a=1, initial_b=1) -> None:\n",
    "        super().__init__(MABproblem)\n",
    "        self.a = np.array([initial_a] * MABproblem.num_arms)\n",
    "        self.b = np.array([initial_b] * MABproblem.num_arms)\n",
    "    \n",
    "    def choose_arm(self) -> int:\n",
    "        samples = [0] * self.MABproblem.num_arms\n",
    "        for i in range(self.MABproblem.num_arms):\n",
    "            samples[i] = np.random.beta(self.a, self.b).sum()\n",
    "        arm = np.argmax(samples)\n",
    "        return arm\n",
    "    \n",
    "    def update_belief(self, arm_choice, reward) -> None:\n",
    "        self.a[arm_choice] += reward\n",
    "        self.b[arm_choice] += 1 - reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fecfd69",
   "metadata": {},
   "source": [
    "##### Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f3f30a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101 176 224] [243 164  98]\n"
     ]
    }
   ],
   "source": [
    "thompson_sampling = ThompsonSamping(FruitMachine)\n",
    "thompson_sampling.run(num_iterations=1000)\n",
    "print(thompson_sampling.a, thompson_sampling.b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "c6e4e9f98eb68ad3b7c296f83d20e6de614cb42e90992a65aa266555a3137d0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
