{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4496662c",
   "metadata": {},
   "source": [
    "# Multi-armed Bandits Algorithms\n",
    "\n",
    "This notebook explains and reproduces the most popular multi-armed bandits (MAB) algorithms:\n",
    "- epsilon-greedy\n",
    "- UCB\n",
    "- Thompson Sampling\n",
    "\n",
    "### What?\n",
    "MAB is a classic reinforcement learning problem for balancing the trade-off between exploration vs exploitation.\n",
    "\n",
    "Essentially, MAB formulates various kinds of sequential decision-making problems; thus, it is applicable to a wide range of scenarios.\n",
    "\n",
    "### Example\n",
    "In a casino, a gambler plays a slot machine that has `K` pulling arms (the story if there are `K` slot machines with one arm). \n",
    "\n",
    "How to leave the casino without being bankrupt? Or even with some earnings?\n",
    "\n",
    "##### Rules\n",
    "1. Each arm has a reward distribution that is initially unknown to the gambler.\n",
    "2. Each round, the gambler can only play one arm and only observe the reward of that very arm.\n",
    "\n",
    "##### Goal\n",
    "Maximize the cumulative reward before he bankrupts or the casino closes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b9d810",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "04befc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bb0f59",
   "metadata": {},
   "source": [
    "# Problem (Bandit Environment)\n",
    "\n",
    "Don't get confused.\n",
    "\n",
    "MAB algorithms solves MAB problems.\n",
    "\n",
    "We need to define a problem first before investigating how each algorithms perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9fc9b5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractMAPProblem(object):\n",
    "    \"\"\"\n",
    "    The basics of a generic MAB problem\n",
    "    \"\"\"\n",
    "    def __init__(self, name: str, num_arms: int):\n",
    "        self.bandit_name = name\n",
    "        self.num_arms = num_arms\n",
    "\n",
    "    def get_reward(self, arm_choice: int):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b304aa",
   "metadata": {},
   "source": [
    "### The gambler, Jack\n",
    "In front of Jack, the gambler, is a fancy slot machine, called the Fruit Machine.\n",
    "```\n",
    "  ______          _ _     __  __            _     _            \n",
    " |  ____|        (_) |   |  \\/  |          | |   (_)           \n",
    " | |__ _ __ _   _ _| |_  | \\  / | __ _  ___| |__  _ _ __   ___ \n",
    " |  __| '__| | | | | __| | |\\/| |/ _` |/ __| '_ \\| | '_ \\ / _ \\\n",
    " | |  | |  | |_| | | |_  | |  | | (_| | (__| | | | | | | |  __/\n",
    " |_|  |_|   \\__,_|_|\\__| |_|  |_|\\__,_|\\___|_| |_|_|_| |_|\\___|\n",
    "```\n",
    "\n",
    "##### Fruit Machine\n",
    "There are 3 arms/buttons that correspond to 3 different fruits (cherry, orange, and manage).\n",
    "\n",
    "Each time Jack pays $1 to play.\n",
    "\n",
    "This machine only requires one single brain cell to play:\n",
    "1. choose a fruit & pull its arm -> the wheel starts to spin -> the wheel stops\n",
    "2. win the prize (get back $2) or lose ($1 is gone)\n",
    "3. play again (going home is never an option for Jack :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "619a9f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FruitSlotMachine(AbstractMAPProblem):\n",
    "    \"\"\"\n",
    "    An Bernoulli Bandit. Reward obeys a Bernoulli distribution\n",
    "    This machine has 3 arms:\n",
    "    0 -- cherry\n",
    "    1 -- orange\n",
    "    2 -- mango\n",
    "    \"\"\"\n",
    "    def __init__(self, distribution: list = None):\n",
    "        \"\"\"\n",
    "        this machine supports two factory modes:\n",
    "        1. set a fixed win rate for each arm\n",
    "        2. randomize the win rate for each arm\n",
    "        \"\"\"\n",
    "        super().__init__(\"Fruit Machine\", 3)\n",
    "        if distribution is not None: # mode 1\n",
    "            assert len(distribution) == self.num_arms\n",
    "            self.distribution = distribution\n",
    "        else: # mode 2\n",
    "            self.distribution = [np.random.random() for _ in range(self.num_arms)]\n",
    "        \n",
    "        self.best_cumulative_reward = max(self.distribution)\n",
    "\n",
    "    def get_reward(self, arm_choice: int):\n",
    "        \"\"\"\n",
    "        return the sampled reward for the given arm choice\n",
    "        \n",
    "        Output:\n",
    "        reward -- 0 or 1\n",
    "        \"\"\"\n",
    "        if np.random.random() < self.distribution[arm_choice]:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92ed314",
   "metadata": {},
   "source": [
    "As we are the owner of the Fruit Machine, we can adjust the win rate:\n",
    "- cherry -> 30% win, 70% lose\n",
    "- orange -> 50% win, 50% lose\n",
    "- manago -> 70% win, 30% lose\n",
    "\n",
    "Obviously, we know betting on manago is always the best.\n",
    "Gamblers need to play tons of times before they eventually realize this secret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2b0340f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Fruit Machine with a fixed win rate / distribution\n",
    "FruitMachine = FruitSlotMachine([0.3, 0.5, 0.7])\n",
    "Fruits = ['cherry', 'orange', 'mango']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9c2f94",
   "metadata": {},
   "source": [
    "# Algorithms\n",
    "An MAB algorithm solves MAB problems.\n",
    "### What does it do?\n",
    "Essentially, an MAB algorithm is a strategy (aka, policy) on how to perform the next move, such that we gain better understanding of the problem and the hidden distributions.\n",
    "The algorithm will:\n",
    "1. `choose_arm`: make a decision on actions (which fruit to bet on?)\n",
    "2. observe the result and `update_belief` based on the arm choice and result/reward.\n",
    "3. repeat for some iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1f7cbcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractMABAlgorithm:\n",
    "    def __init__(self, MABproblem: FruitSlotMachine) -> None:\n",
    "        self.MABproblem = MABproblem\n",
    "        self.N_a = np.array([0] * self.MABproblem.num_arms) # the number of choice on arm a\n",
    "        self.regret = 0 # cumulative regret\n",
    "        self.regret_history = [0] # regret history at each step\n",
    "    \n",
    "    def choose_arm(self) -> int:\n",
    "        \"\"\"choose an arm/action\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def update_belief(self, arm_choice, reward) -> None:\n",
    "        \"\"\"update the model\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def update_regret(self, arm_choice: int):\n",
    "        self.regret += self.MABproblem.best_cumulative_reward - self.MABproblem.distribution[arm_choice]\n",
    "        self.regret_history.append(self.regret)\n",
    "\n",
    "    def run(self, num_iterations: int):\n",
    "        for self.iteration in range(num_iterations):\n",
    "            # choose an arm & observe reward\n",
    "            arm_choice = self.choose_arm()\n",
    "            reward = self.MABproblem.get_reward(arm_choice)\n",
    "            # update model & regret\n",
    "            self.N_a[arm_choice] += 1\n",
    "            self.update_belief(arm_choice, reward)\n",
    "            self.update_regret(arm_choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ca9846",
   "metadata": {},
   "source": [
    "### epsilon-greedy\n",
    "explores a random arm at a probability of ε, and exploit the current best arm otherwise, i.e. greedy choice.\n",
    "\n",
    "##### epsilon value\n",
    "Cesa-Bianchi and Fischer (1998) proved that the regret has logarithmic bound if ε is proportionally to the reciprocal of iterations (i.e. ε ∝ 1t ), though a widely-cited empirical evaluation was not able to prove the practicality of this theoretically sound statement (Vermorel and Mohri, 2005)\n",
    "\n",
    "We use fixed a epsilon value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c3190a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedy(AbstractMABAlgorithm):\n",
    "    def __init__(self, MABproblem: FruitSlotMachine, epsilon: float, initial_probability: float=1.0) -> None:\n",
    "        super().__init__(MABproblem)\n",
    "        assert epsilon is None or 0 <= epsilon <= 1.0\n",
    "        self.epsilon = epsilon\n",
    "        self.probabilities = np.array([initial_probability] * self.MABproblem.num_arms)\n",
    "\n",
    "    def choose_arm(self) -> int:\n",
    "        if np.random.random() < self.epsilon:\n",
    "            arm = np.random.randint(0, self.MABproblem.num_arms)\n",
    "        else:\n",
    "            arm = self.probabilities.argmax()\n",
    "        return arm\n",
    "\n",
    "    def update_belief(self, arm_choice, reward) -> None:\n",
    "        self.probabilities[arm_choice] += (reward - self.probabilities[arm_choice]) / (self.N_a[arm_choice]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a474172c",
   "metadata": {},
   "source": [
    "##### Performance\n",
    "Jack, the gambler, plays for a 1000 times following the Epsilon Greedy algorithm and he keeps track of the winrate of each fruit/arm/action.\n",
    "\n",
    "Jack chooses to evenly splite his exploration and exploitation (50% - 50%. Approaximately 500 times for choosing randomly)\n",
    "\n",
    "Thus, `epsilon = 0.5`\n",
    "\n",
    "Jack uses epsilon greedy to successfully approximates the winrate for each fruit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f892e030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.26108374 0.43195266 0.70382166]\n",
      "Approximate winrate for cherry: 0.261\n",
      "Approximate winrate for orange: 0.432\n",
      "Approximate winrate for mango: 0.704\n"
     ]
    }
   ],
   "source": [
    "epsilon_greedy = EpsilonGreedy(FruitMachine, epsilon=0.5)\n",
    "epsilon_greedy.run(num_iterations=1000)\n",
    "print(epsilon_greedy.probabilities)\n",
    "for i, winrate in enumerate(epsilon_greedy.probabilities):\n",
    "    print(f\"Approximate winrate for {Fruits[i]}:\", round(winrate, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac6afe0",
   "metadata": {},
   "source": [
    "##### Questions\n",
    "We are curious about the following questions:\n",
    "1. How does `epsilon` affect the result? Is larger or smaller `epsilon` works better?\n",
    "2. How does `# iterations` affect the result? In other word, how does the cumulative regret change over time?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615f4865",
   "metadata": {},
   "source": [
    "### Upper Confidence Bounds (UCB)\n",
    "The basic algorithm, UCB1, tracks the number of trails n on each arm besides the expected value μ. The agent selects an arm a based on the optimistic estimates (i.e. maximum of UCB):\n",
    "\n",
    "Repetitively selecting the arm that maximizes UCB will eventually converge to the best arm a∗, which is easily proved by contradiction of UCBt(at) ≥ UCBt(a∗)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cd47d60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCB(AbstractMABAlgorithm):\n",
    "    def __init__(self, MABproblem: FruitSlotMachine, initial_probability: float=1.0) -> None:\n",
    "        super().__init__(MABproblem)\n",
    "        self.probabilities = np.array([initial_probability] * self.MABproblem.num_arms)\n",
    "\n",
    "    def choose_arm(self) -> int:\n",
    "        t = self.iteration+1\n",
    "        UCB = self.probabilities + np.sqrt(2 * np.log(t) / (self.N_a + 1))\n",
    "        arm = np.argmax(UCB)\n",
    "        return arm\n",
    "\n",
    "    def update_belief(self, arm_choice, reward) -> None:\n",
    "        self.probabilities[arm_choice] += (reward - self.probabilities[arm_choice]) / (self.N_a[arm_choice]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af694e14",
   "metadata": {},
   "source": [
    "##### Performance\n",
    "UCB can also help Jack successfully approximates the winrate for each fruit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "84819a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.32323232 0.50961538 0.7091459 ]\n",
      "Approximate winrate for cherry: 0.323\n",
      "Approximate winrate for orange: 0.51\n",
      "Approximate winrate for mango: 0.709\n"
     ]
    }
   ],
   "source": [
    "ucb = UCB(FruitMachine)\n",
    "ucb.run(num_iterations=1000)\n",
    "print(ucb.probabilities)\n",
    "for i, winrate in enumerate(ucb.probabilities):\n",
    "    print(f\"Approximate winrate for {Fruits[i]}:\", round(winrate, 3))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "c6e4e9f98eb68ad3b7c296f83d20e6de614cb42e90992a65aa266555a3137d0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
